# <p align=center>Interpretable Cross-modal Reasoning


> [**A Survey on Interpretable Cross-modal Reasoning**](https://arxiv.org/abs/2309.01955)<br>
[Dizhan Xue](https://orcid.org/0000-0002-0173-1556)[Shengsheng Qian](https://orcid.org/0000-0001-9488-2208) [Zuyi Zhou](https://orcid.org/0009-0008-0306-8461) [Changsheng Xu]
> **<p align="justify"> Abstract:** *In recent years, cross-modal reasoning (CMR), the process of understanding and reasoning across different modalities, has emerged as a  pivotal area with applications spanning from multimedia analysis to healthcare diagnostics. As the deployment of AI systems becomes  more ubiquitous, the demand for transparency and comprehensibility in these systemsâ€™ decision-making processes has intensified. This survey delves into the realm of interpretable cross-modal reasoning (I-CMR), where the objective is not only to achieve high  predictive performance but also to provide human-understandable explanations for the results. This survey presents a comprehensive  overview of the typical methods with a three-level taxonomy for I-CMR. Furthermore, this survey reviews the existing CMR datasets  with annotations for explanations. Finally, this survey summarizes the challenges for I-CMR and discusses potential future directions. In conclusion, this survey aims to catalyze the progress of this emerging research area by providing researchers with a panoramic and  comprehensive perspective, illuminating the state of the art and discerning the opportunities.* </p>

##  Citation

If you find this paper and repo helpful for your research, please cite it as below:

```bibtex
@misc{xue2023survey,
      title={A Survey on Interpretable Cross-modal Reasoning}, 
      author={Dizhan Xue and Shengsheng Qian and Zuyi Zhou and Changsheng Xu},
      year={2023},
      eprint={2309.01955},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

```
## <span id="head-content"> *Content* </span>
* - [x] [1. Introduction](#headIntro)

* - [ ] [2. Methods for interpretable cross-modal reasoning](#headtax)
  * - [x] [2.1 Methods of Visual Explanation](#head-1)
  * - [x] [2.2 Methods of Textual Explanation](#head-2)  
  * - [x] [2.3 Methods of Graph Explanation](#head-3)
  * - [x] [2.4 Methods of Symbol Explanation](#head-4)
  * - [x] [2.5 Methods of Multimodal Explanation](#head-5)
  
  
* - [ ] [3. Datasets](#headda)  
  * - [x] [Oxford-102 Flower](#head-flower)
  * - [x] [Caltech-UCSD Bird (CUB)](#head-cub)
  * - [x] [MS-COCO](#head-coco)


* [*Contact Us*](#head7)
## <span id="headIntro"> *1. Description* </span>

* ...

* Papers, codes, and datasets for the text-to-image task are available here.


## <span id="headtax"> *2. Methods for interpretable cross-modal reasoning* </span> [       Â«ðŸŽ¯Back To TopÂ»       ](#)

### <span id="head-1"> *2.1 Methods of Visual Explanation* </span>


### <span id="head-2"> *2.2 Methods of Textual Explanation* </span>


### <span id="head-3"> *2.3 Methods of Graph Explanation* </span>


### <span id="head-4"> *2.4 Methods of Symbol Explanation* </span>


### <span id="head-5"> *2.5 Methods of Multimodal Explanation* </span>

## <span id="headtax"> *3. Datasets* </span> [       Â«ðŸŽ¯Back To TopÂ»       ](#)